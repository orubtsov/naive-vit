# Simple Vision Transformer (ViT)

This repository contains notebooks and code for a step-by-step implementation of a simplified Vision Transformer (ViT), based on the paper "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"


The project is structured to progressively implement core components of ViT from scratch, illustrating one concept at a time.

## ğŸ“ Contents

- `notebooks/` â€” Jupyter notebooks for interactive development and visual exploration
- `models.py` â€” Core model components (patch embedding, attention, transformer blocks)

## ğŸ§  What You'll Learn

- Patch embedding with linear projections
- Sinusoidal and learned positional encodings
- Multi-head self-attention from scratch
- Transformer blocks for vision
- Classification head and training loop
- CLS token

## ğŸš€ How to Run

1. Install dependencies:

    ```bash
    pip install -r requirements.txt
    ```

2. Run notebooks one by one inside the `notebooks/` folder.

## ğŸ“Œ Notes

This is an educational project focused on clarity and conceptual understanding, not performance or completeness.