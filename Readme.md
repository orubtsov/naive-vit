# Simple Vision Transformer (ViT)

This repository contains notebooks and code for a step-by-step implementation of a simplified Vision Transformer (ViT), based on the paper "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"


The project is structured to progressively implement core components of ViT from scratch, illustrating one concept at a time.

## 📁 Contents

- `notebooks/` — Jupyter notebooks for interactive development and visual exploration
- `models.py` — Core model components (patch embedding, attention, transformer blocks)

## 🧠 What You'll Learn

- Patch embedding with linear projections
- Sinusoidal and learned positional encodings
- Multi-head self-attention from scratch
- Transformer blocks for vision
- Classification head and training loop
- CLS token

## 🚀 How to Run

1. Install dependencies:

    ```bash
    pip install -r requirements.txt
    ```

2. Run notebooks one by one inside the `notebooks/` folder.

## 📌 Notes

This is an educational project focused on clarity and conceptual understanding, not performance or completeness.